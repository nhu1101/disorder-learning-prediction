{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dd8e4a5-f05e-4a08-a770-6efea2017bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, model_type, X, features, score_col, index_col, cv, sample_weights=False):\n",
    "    # Parameter space to explore if model is xgboost\n",
    "    if model_type == 'xgboost':\n",
    "        params = {\n",
    "            'objective': trial.suggest_categorical('objective', ['reg:tweedie', 'reg:pseudohubererror']),\n",
    "            'random_state': SEED,\n",
    "            'num_parallel_tree': trial.suggest_int('num_parallel_tree', 2, 30),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 4),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 0.02, 0.05),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 0.8),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.8),\n",
    "            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 1e-1),\n",
    "            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 1e-1),\n",
    "        }\n",
    "        if params['objective'] == 'reg:tweedie':\n",
    "            params['tweedie_variance_power'] = trial.suggest_float('tweedie_variance_power', 1, 2)\n",
    "        model = XGBRegressor(**params, use_label_encoder=False)\n",
    "    \n",
    "    # Parameter space to explore if model is lightgbm\n",
    "    elif model_type == 'lightgbm':\n",
    "        params = {\n",
    "            'objective': trial.suggest_categorical('objective', ['poisson', 'tweedie', 'regression']),\n",
    "            'random_state': SEED,\n",
    "            'verbosity': -1,\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 4),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.05),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 0.8),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.8),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 100)\n",
    "        }\n",
    "        if params['objective'] == 'tweedie':\n",
    "            params['tweedie_variance_power'] = trial.suggest_float('tweedie_variance_power', 1, 2)\n",
    "        model = LGBMRegressor(**params)\n",
    "    \n",
    "    # Parameter space to explore if model is catboost\n",
    "    elif model_type == 'catboost':\n",
    "        params = {\n",
    "            'loss_function': trial.suggest_categorical('objective', ['Tweedie:variance_power=1.5', \n",
    "                                                                     'Poisson', 'RMSE']),\n",
    "            'random_state': SEED,\n",
    "            'iterations': trial.suggest_int('iterations', 100, 300),\n",
    "            'depth': trial.suggest_int('depth', 2, 4),\n",
    "            'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.05),\n",
    "            'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 1e-1),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 0.7),\n",
    "            'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "            'random_strength': trial.suggest_float('random_strength', 1e-3, 10.0),\n",
    "            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 20, 60),\n",
    "        }\n",
    "        model = CatBoostRegressor(**params, verbose=0)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model_type: {model_type}\")\n",
    "        \n",
    "    seeds = [random.randint(1, 10000) for _ in range(20)]\n",
    "\n",
    "    score, _ = n_cross_validate(model, X, features, score_col, index_col, cv, seeds, sample_weights=True, verbose=True)\n",
    "\n",
    "    return score\n",
    "\n",
    "def run_optimization(X, features, score_col, index_col, model_type, n_trials=30, cv=None, sample_weights=False):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, model_type, X, features, score_col, index_col, cv, sample_weights), \n",
    "                   n_trials=n_trials)\n",
    "    \n",
    "    print(f\"Best params for {model_type}: {study.best_params}\")\n",
    "    print(f\"Best score: {study.best_value}\")\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3349fc30-4244-409c-a1bf-46b24907ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = [\"PC_9\", \"PC_12\", \"Fitness_Endurance-Max_Stage\", \"Basic_Demos-Sex\", 'BMI_mean_norm', \"PC_11\", \"PC_8\", \"FGC_Zones_min\", 'Physical-Systolic_BP',\n",
    "           \"PC_4\", \"BIA-BIA_FMI\", \"BIA-BIA_LST\", \"Physical-Diastolic_BP\", 'BIA-BIA_ECW', 'Fitness_Endurance-Time_Mins', 'PAQ_C-PAQ_C_Total', 'PC_10',\n",
    "           'BIA-BIA_Fat', 'FFM_norm', 'PC_14', 'PC_7']\n",
    "\n",
    "reduced_features = [f for f in features if f not in exclude]\n",
    "\n",
    "lgb_features = reduced_features\n",
    "xgb_features = reduced_features\n",
    "cat_features = reduced_features\n",
    "print(len(reduced_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da635ba6-b49e-48f4-a68d-d8d998905a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for LGBM, XGB and CatBoost\n",
    "lgb_params = {\n",
    "    'objective': 'poisson', \n",
    "    'n_estimators': 295, \n",
    "    'max_depth': 4, \n",
    "    'learning_rate': 0.04505693066482616, \n",
    "    'subsample': 0.6042489155604022, \n",
    "    'colsample_bytree': 0.5021876720502726, \n",
    "    'min_data_in_leaf': 100\n",
    "}\n",
    "\n",
    "xgb_params = {'objective': 'reg:tweedie', 'num_parallel_tree': 12, 'n_estimators': 236, 'max_depth': 3, 'learning_rate': 0.04223740904479563, 'subsample': 0.7157264603586825, 'colsample_bytree': 0.7897918901977528, 'reg_alpha': 0.005335705058190553, 'reg_lambda': 0.0001897435318347022, 'tweedie_variance_power': 1.1393958601390142}\n",
    "\n",
    "xgb_params_2 = {\n",
    "    'objective': 'reg:tweedie', \n",
    "    'num_parallel_tree': 18, \n",
    "    'n_estimators': 175, \n",
    "    'max_depth': 3, \n",
    "    'learning_rate': 0.032620453423049305, \n",
    "    'subsample': 0.6155579670568023, \n",
    "    'colsample_bytree': 0.5988773292417443, \n",
    "    'reg_alpha': 0.0028895066837627205, \n",
    "    'reg_lambda': 0.002232531512636924, \n",
    "    'tweedie_variance_power': 1.1708678482038286\n",
    "}\n",
    "\n",
    "cat_params = {\n",
    "    'objective': 'RMSE', \n",
    "    'iterations': 238, \n",
    "    'depth': 4, \n",
    "    'learning_rate': 0.044523361750173816, \n",
    "    'l2_leaf_reg': 0.09301285673435761, \n",
    "    'subsample': 0.6902492783438681, \n",
    "    'bagging_temperature': 0.3007304771330199, \n",
    "    'random_strength': 3.562201626987314, \n",
    "    'min_data_in_leaf': 60\n",
    "}\n",
    "\n",
    "xtrees_params = {\n",
    "    'n_estimators': 500, \n",
    "    'max_depth': 15, \n",
    "    'min_samples_leaf': 20, \n",
    "    'bootstrap': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb19ac95-5a3b-41d2-9268-77215e0b6ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "if optimize_params:\n",
    "    # LightGBM Optimization\n",
    "    lgb_params = run_optimization(train, lgb_features, 'PCIAT-PCIAT_Total', 'sii', 'lightgbm', n_trials=n_trials, cv=kf, sample_weights=True)\n",
    "\n",
    "    # XGBoost Optimization\n",
    "    xgb_params = run_optimization(train, xgb_features, 'PCIAT-PCIAT_Total', 'sii', 'xgboost', n_trials=n_trials, cv=kf, sample_weights=True)\n",
    "\n",
    "    # CatBoost Optimization\n",
    "    cat_params = run_optimization(train, cat_features, 'PCIAT-PCIAT_Total', 'sii', 'catboost', n_trials=n_trials, cv=kf, sample_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5a0181-cb6c-4ddc-b826-17a4bf8f87df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "lgb_model = LGBMRegressor(**lgb_params, random_state=SEED, verbosity=-1)\n",
    "xgb_model = XGBRegressor(**xgb_params, random_state=SEED, verbosity=0)\n",
    "xgb_model_2 = XGBRegressor(**xgb_params_2, random_state=SEED, verbosity=0)\n",
    "cat_model = CatBoostRegressor(**cat_params, random_state=SEED, verbose=0)\n",
    "xtrees_model = ExtraTreesRegressor(**xtrees_params, random_state=SEED)\n",
    "\n",
    "weights = calculate_weights(train['PCIAT-PCIAT_Total'])\n",
    "\n",
    "# Cross-validate LGBM model\n",
    "score_lgb, oof_lgb, lgb_thresholds = cross_validate(\n",
    "    lgb_model, train, lgb_features, 'PCIAT-PCIAT_Total', 'sii', kf, verbose=True, sample_weights=True\n",
    ")\n",
    "lgb_model.fit(train[lgb_features], train['PCIAT-PCIAT_Total'], sample_weight=weights)\n",
    "test_lgb = lgb_model.predict(test[lgb_features])\n",
    "\n",
    "# Cross-validate XGBoost model\n",
    "score_xgb, oof_xgb, xgb_thresholds = cross_validate(\n",
    "    xgb_model, train, xgb_features, 'PCIAT-PCIAT_Total', 'sii', kf, verbose=True, sample_weights=True\n",
    ")\n",
    "xgb_model.fit(train[xgb_features], train['PCIAT-PCIAT_Total'], sample_weight=weights)\n",
    "test_xgb = xgb_model.predict(test[xgb_features])\n",
    "\n",
    "# Cross-validate XGBoost model 2\n",
    "score_xgb_2, oof_xgb_2, xgb_2_thresholds = cross_validate(\n",
    "    xgb_model_2, train, xgb_features, 'PCIAT-PCIAT_Total', 'sii', kf, verbose=True, sample_weights=True\n",
    ")\n",
    "xgb_model_2.fit(train[xgb_features], train['PCIAT-PCIAT_Total'], sample_weight=weights)\n",
    "test_xgb_2 = xgb_model_2.predict(test[xgb_features])\n",
    "\n",
    "# Cross-validate CatBoost model\n",
    "score_cat, oof_cat, cat_thresholds = cross_validate(\n",
    "    cat_model, train, cat_features, 'PCIAT-PCIAT_Total', 'sii', kf, verbose=True, sample_weights=True\n",
    ")\n",
    "cat_model.fit(train[cat_features], train['PCIAT-PCIAT_Total'], sample_weight=weights)\n",
    "test_cat = cat_model.predict(test[cat_features])\n",
    "\n",
    "# Cross-validate ExtraTreesRegressor model\n",
    "score_xtrees, oof_xtrees, xtrees_thresholds = cross_validate(\n",
    "    xtrees_model, train, reduced_features, 'PCIAT-PCIAT_Total', 'sii', kf, verbose=True, sample_weights=True\n",
    ")\n",
    "xtrees_model.fit(train[reduced_features], train['PCIAT-PCIAT_Total'], sample_weight=weights)\n",
    "test_xtrees = xtrees_model.predict(test[reduced_features])\n",
    "\n",
    "# Print overall mean Kappa score for all models\n",
    "print(f'Overall Mean Kappa: {np.mean([score_lgb, score_xgb, score_cat, score_xtrees])}') # Ensemble score likely higher"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
